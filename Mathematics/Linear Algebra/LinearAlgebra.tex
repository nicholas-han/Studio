\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{extarrows}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{tikz}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\newtheorem{axiom}{Axiom}[section] % a basic assumption about a mathematical situation
\newtheorem{definition}{Definition}[section] % an explanation of the mathematical meaning of a word
\newtheorem{notation}{Notation}[section] % an equivalent manifestation introduced to simplify mathematical writings
\newtheorem{example}{Example}[section]
\newtheorem{conjecture}{Conjecture}[section] % a statement believed to be true, but not yet proved
\newtheorem{theorem}{Theorem}[section] % a statement that has been proven to be true
\newtheorem{proposition}{Proposition}[section] % a less important but nonetheless interesting true statement
\newtheorem{lemma}{Lemma}[section] % a less important theorem that is helpful in proving other true statements
\newtheorem{corollary}{Corollary}[section] % a true statment that is a simple deduction from a theorem or proposition
\newtheorem{property}{Property}[section] % an obvious conclusion deducted from a theorem or proposition

\setstretch{1.2}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}

% ------------------
\begin{document}
% ------------------
% Cover Page and ToC
% ------------------
\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{\uppercase{Linear Algebra}
		\HRule{2.0pt} \\ [0.6cm] \vspace*{10\baselineskip}}}

\date{}
\author{\textbf{Author:} Nicholas Han \\ Shenyang 2025}

\maketitle
\thispagestyle{empty}
\newpage

\tableofcontents
\thispagestyle{empty}
\newpage
% ------------------


\pagenumbering{arabic}
\section{Linear Equations}

\begin{definition}[Linear Equation]
    A \textbf{linear equation} is an equation of variables of the first order. It may be put in the form of \(a_1x_1+a_2x_2+...+a_nx_n=b\), where \(x_1, x_2, ..., x_n\) are \textbf{variables} (or \textbf{unknowns}), \(a_1, a_2, ..., a_n\) are \textbf{coefficients}, and \(b\) is \textbf{constant}.
\end{definition}

\begin{definition}[System of Linear Equations]
    A \textbf{system of linear equations} (or a \textbf{linear system}) is a collection of one or more linear equations involving the same variables. It may be put in the form
    \begin{equation}\label{def_lin_eqn}
    \begin{split}
        a_{11}x_1+a_{12}x_2&+...+a_{1n}x_n=b_1,\\
        a_{21}x_1+a_{22}x_2&+...+a_{2n}x_n=b_2,\\
        &......\\
        a_{s1}x_1+a_{s2}x_2&+...+a_{sn}x_n=b_s\\
    \end{split}
    \end{equation}
    where \(a_{11}, a_{12}, ..., a_{sn}\) are \textbf{coefficients}, and \(b_1, b_2, ..., b_s\) are \textbf{constants}. And the number of equations \textbf{s} can be equal to, greater than, or less than the number of variables \textbf{n}.
\end{definition}

\begin{definition}[Solutions to a System of Linear Equations]
If there exist \(c_1, c_2, ..., c_n\) that can be plugged into \(x_1, x_2, ..., x_n\) so that the linear equations in (\ref{def_lin_eqn}) hold, the ordered list of \((c_1, c_2, ..., c_n)\) is called a \textbf{solution} to the system of linear equations. The set of all solutions is called the \textbf{solution set}.
\end{definition}

\subsection{The Geometry of Linear Equations}
Suppose we have a system of 2 linear equations with 2 variables.
    \begin{equation}\label{ex_lin_eqn}
    \begin{split}
    2x-y&=0 \\
    -x+2y&=3
    \end{split}
    \end{equation}
\subsubsection{Geometric Representation (by Rows)}
Each linear equation represents a line in the xy plane, and the intersection of the 2 lines (if there is one) represents the solution (in this case the only solution) to the system of equations, which is \(x=1\), \(y=2\).

\begin{center}
\begin{tikzpicture}
    \begin{axis}[grid=both,ymin=0,ymax=5,xmin=0,xmax=5,minor tick num=1,axis lines = middle,xlabel=$x$,ylabel=$y$,label style ={at={(ticklabel cs:1.1)}}]
        \addplot[domain=0:5,color=red]{2*x};
        \addplot[domain=0:5,color=blue]{x/2+1.5};
        \addplot+[only marks] coordinates {(1,2)};
        \node[color=red] at (axis cs: 3,4) {$2x-y=0$};
        \node[color=blue] at (axis cs: 3.5,2.5) {$-x+2y=3$};
        \node[color=black] at (axis cs: 1.5,1.7) {$(1,2)$};
    \end{axis}
\end{tikzpicture}
\end{center}

This representation of linear equations corresponds to that of analytic geometry that we studied in high school.

\subsubsection{Vector Representation (by Columns)}
\begin{notation}[Column Vector] A column vector $\vec{x}$ consisting of \textbf{m} elements is denoted as
\begin{center}
    \begin{equation}\label{col_vec}
    \begin{split}
    \vec{x}=\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \;
    \end{split}
    \end{equation}
\end{center}
\end{notation}

We can thus rewrite the linear equations in (\ref{ex_lin_eqn}) by turning the coefficients into column vectors.
\begin{center}
\begin{equation}
    x \begin{bmatrix} \phantom{-} 2 \\ -1 \end{bmatrix} + y \begin{bmatrix} -1 \\ \phantom{-} 2 \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \end{bmatrix}
\end{equation}
\end{center}

We want to find numbers $x$ and $y$ so that $x$ copies of vector $\begin{bmatrix} \phantom{-} 2 \\ -1 \end{bmatrix}$ added to $y$ copies of vector $\begin{bmatrix} -1 \\ \phantom{-} 2 \end{bmatrix}$ equals to $\begin{bmatrix} 0 \\ 3 \end{bmatrix}$.

\begin{center}
\begin{tikzpicture}[scale=1.5]
    % Draw coordinate axes
    \draw[->] (-2,0) -- (3,0) node[right] {$x$};
    \draw[->] (0,-2) -- (0,3) node[above] {$y$};
    
    % Draw grid
    \draw[gray!30] (-2,-2) grid (2.5,4);
    
    % Draw vectors
    \draw[->, thick, blue] (0,0) -- (2,-1) node[midway, below] {$\vec{v}_1 = \begin{bmatrix} \phantom{-} 2 \\ -1 \end{bmatrix}$};
    \draw[->, thick, red] (0,0) -- (-1,2) node[midway, below left] {$\vec{v}_2 = \begin{bmatrix} -1 \\ \phantom{-} 2 \end{bmatrix}$};
    \draw[->, thick, green!50!black] (0,0) -- (0,3) node[midway, right] {$\vec{v}_3 = \begin{bmatrix} 0 \\ 3 \end{bmatrix}$};
    \draw[->, densely dashed, red] (2,-1) -- (1,1) node[midway, above right] {$\begin{bmatrix} -1 \\ \phantom{-} 2 \end{bmatrix}$};
    \draw[->, densely dashed, red] (1,1) -- (0,3) node[midway, above right] {$\begin{bmatrix} -1 \\ \phantom{-} 2 \end{bmatrix}$};

    % Add some points for reference
    \foreach \x in {-2,-1,1,2}
        \draw (\x,0.1) -- (\x,-0.1) node[below] {$\x$};
    \foreach \y in {-2,-1,1,2}
        \draw (0.1,\y) -- (-0.1,\y) node[left] {$\y$};
\end{tikzpicture}
\end{center}

This representation of linear equations corresponds to that of vectors that we studied in high school.

\subsubsection{Matrix Representation (by Rows and Columns)}
Is there a more holistic way to view a system of linear equations? Yes, by using a matrix. Let's wait until we introduce the idea of matrix in the next chapter.



\subsection{Solving Linear Equations}
To solve a system of linear equations, we use the technique of \textit{elimination}. The three basic elimination operations are
\begin{enumerate}
    \item Add two linear equations
    \item Swap two linear equations
    \item Multiply a linear equation by a non-zero number
\end{enumerate}
The solutions to the system of linear equations remain unchanged after the elimination operations.

\begin{example}\label{ex_elim}
    Suppose we have the following linear equations. We can apply the elimination operations step-by-step. 
    \begin{align}
        x_1 + & 2x_2 + x_3 = 2 \\ \notag
        3x_1 + & 8x_2 + x_3 = 12 \\ \notag
        & 4x_2 + x_3 = 2 \notag
    \end{align}
    $\xrightarrow{\text{(2) = (2) - 3 $\cdot$ (1)}}$
    \begin{align}
        x_1 + & 2x_2 + x_3 = 2 \\ \notag
        & 2x_2 - 2x_3 = 6 \\ \notag
        & 4x_2 + x_3 = 2
    \end{align}
    $\xrightarrow{\text{(3) = (3) - 2 $\cdot$ (2)}}$
    \begin{align}
        x_1 + 2x_2 + & x_3 = 2 \\ \notag
        2x_2 - & 2x_3 = 6 \\ \notag
        & 5x_3 = -10
    \end{align}
    $\xrightarrow{\text{(3) = (3)/5 }}$
    \begin{align}
        x_1 + 2x_2 + & x_3 = 2 \\ \notag
        2x_2 - & 2x_3 = 6 \\ \notag
        & x_3 = -2
    \end{align}
    Then by back substitution, we can easily obtain $x_3=-2$, $x_2=1$, and $x_1=2$.
\end{example}

\section{Matrices}
% 丘维声 chapter 1 page 4
% MIT notes的matrix multiplication

Taking one more step forward, we write the linear equations in (\ref{ex_lin_eqn}) as a single equation using matrices and vectors.

\begin{center}
\begin{equation}
    \begin{bmatrix} \phantom{-} 2 -1 \\ -1 \phantom{-} 2 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \end{bmatrix}
\end{equation}
\end{center}

\begin{definition}
    A table of s $\cdot$ n numbers displayed in the form of s rows and n columns is called an \textbf{s $\times$ n matrix}, with each number called an \textbf{element} or an \textbf{entry} of the matrix. \\
    A matrix with all 0 entries is called a \textbf{zero matrix}. \\
    A matrix with $s=n$ is called a \textbf{square matrix}; an m $\times$ m matrix is called a \textbf{square matrix of degree m}.
\end{definition}

\begin{notation}[Matrix Representation]
    Given the linear equations (\ref{def_lin_eqn}), we denote the \textbf{coefficient matrix} as matrix
    $A = \begin{bmatrix}
        a_{11} \ a_{12} & ... \ a_{1n} \\
        a_{21} \ a_{22} & ... \ a_{2n} \\
        ...... \\
        a_{s1} \ a_{s2} & ... \ a_{sn} \\
    \end{bmatrix}$. The vector $\textbf{x}=\begin{bmatrix}x_1 \\ x_2 \\ ... \\ x_n \end{bmatrix}$ is the vector of unknowns. The values on the RHS of the equations are denoted as $\textbf{b}=\begin{bmatrix}b_1 \\ b_2 \\ ... \\ b_n \end{bmatrix}$. Thus, we have $A\textbf{x}=\textbf{b}$.
    We can also write the coefficient vector \textbf{b} next to matrix $A$ in the form of $[A | \textbf{b}]=\begin{bmatrix}
    \begin{array}{cccc|c}
        a_{11} & a_{12} & ... & a_{1n} & b_1 \\
        a_{21} & a_{22} & ... & a_{2n} & b_2 \\
        ... & ... & ... & ... & ... \\
        a_{s1} & a_{s2} & ... & a_{sn} & b_n \\
    \end{array}
    \end{bmatrix}$, called the \textbf{augmented matrix}.
\end{notation}


\subsection{Revisit: Solving Linear Equations (in Matrix Representation)}
If the system of linear equations is denoted in matrix representation, we can correspondingly use \textit{elimination matrices} and \textit{permutation matrices} to perform the elimination operations.

\begin{notation}
    An elimination matrix \textit{$E_{ij}$} takes the form of an identity matrix with one non-zero entry off the main diagonal, say \textit{k}, at the entry of \textit{(i,j)}, such that it performs the operation of adding \textit{k} times of row \textit{j} to row \textit{i}.
\end{notation}

For \ref{ex_elim}, we can therefore write the elimination operations as
\begin{equation}
    E_{12}=
    \begin{bmatrix}
        \begin{array}{ccc}
            1 & 0 & 0 \\
            -3 & 1 & 0 \\
            0 & 0 & 1
        \end{array}
    \end{bmatrix}, E_{23}=
    \begin{bmatrix}
        \begin{array}{ccc}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & -2 & 1
        \end{array}
    \end{bmatrix}
\end{equation}

That is, $E_{23}(E_{12}A)=U$.
\begin{equation}
    \begin{bmatrix}
        \begin{array}{ccc}
            1 & 0 & 0 \\
            -3 & 1 & 0 \\
            0 & 0 & 1
        \end{array}
    \end{bmatrix}
    \begin{bmatrix}
        \begin{array}{ccc}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & -2 & 1
        \end{array}
    \end{bmatrix}
    \begin{bmatrix}
        \begin{array}{ccc}
            1 & 2 & 1 \\
            3 & 8 & 1 \\
            0 & 4 & 1
        \end{array}
    \end{bmatrix}=
    \begin{bmatrix}
        \begin{array}{ccc}
            1 & 2 & 1 \\
            0 & 2 & -2 \\
            0 & 0 & 1
        \end{array}
    \end{bmatrix}
\end{equation}

A permutation matrix P takes the form of an identity matrix swapping two rows, and performs the task of exchanging two rows of a matrix.
\begin{equation}
    \begin{bmatrix}
        \begin{array}{ccc}
            0 & 1 & 0 \\
            1 & 0 & 0 \\
            0 & 0 & 1
        \end{array}
    \end{bmatrix}
\end{equation}

This approach is called the Gauss-Jordan Elimination, where Gauss's elimination method converts the original matrix to upper triangular form, and Jordan's method further eliminates the entries in the upper right side of the matrix.





\newpage
\section{Vector Spaces}
\fbox{
    \parbox{0.8\textwidth}{
        Think of a field as a generalization of $\mathbb{R}$. \\
        Think of a vector space as a generalization of $\mathbb{R}^2$, $\mathbb{R}^3$, ...
    }
}
\subsection{Fields}
\begin{definition}[Binary Operation]
    Let $\mathbb{F}$ be a set. A \textbf{binary operation} is a function $\mathbb{F}$ $\times$ $\mathbb{F}$ $\rightarrow$ $\mathbb{F}$.
\end{definition}
\begin{example}
    Addition on real numbers is a binary operation.
\end{example}

\begin{definition}[Field]
    A \textbf{field $\mathbb{F}$} is a set together with two binary operations \textbf{addition} and \textbf{multiplication}, such that the following properties hold:
    \begin{enumerate}[label=(\arabic*)]
        \item $\forall \ \alpha, \beta \in \mathbb{F}, \ \alpha+\beta=\beta+\alpha \ (additive \ commutativity)$
        \item $\forall \ \alpha, \beta, \gamma \in \mathbb{F}, \ \alpha+(\beta+\gamma)=(\alpha+\beta)+\gamma \ (additive \ associativity)$
        \item $\forall \ \alpha, \beta \in \mathbb{F}, \ \alpha \cdot \beta = \beta \cdot \alpha \ (multiplicative \ commutativity)$
        \item $\forall \ \alpha, \beta, \gamma \in \mathbb{F}, \ \alpha \cdot (\beta \cdot \gamma)=(\alpha \cdot \beta) \cdot \gamma \ (multiplicative \ associativity)$
        \item $\forall \ \alpha, \beta, \gamma \in \mathbb{F}, \ \alpha \cdot (\beta + \gamma)=\alpha \cdot \beta + \alpha \cdot \gamma \ (distributivity)$
        \item $\exists \ 0 \in \mathbb{F}, \ s.t. \ \forall \ \alpha \in \mathbb{F}, \ 0 + \alpha = \alpha \ (additive \ identity)$
        \item $\forall \ \alpha \in \mathbb{F}, \ \exists \ -\alpha \in \mathbb{F}, \ s.t. \ \alpha + (-\alpha) = 0 \ (additive \ inverse)$
        \item $\exists \ 1 \in \mathbb{F}, \ s.t. \ \forall \ \alpha \in \mathbb{F}, \ 1 \cdot \alpha = \alpha \ (multiplicative \ identity)$
        \item $\forall \ \alpha \in \mathbb{F} \ \& \ \alpha \neq 0, \ \exists \ {\alpha}^{-1} \in \mathbb{F}, \ s.t. \ \alpha \cdot {\alpha}^{-1} = 1 \ (multiplicative \ inverse)$
    \end{enumerate}
\end{definition}


\begin{example} \
    \begin{enumerate}
        \item $\mathbb{Z}$ is not a field, because the property of \textit{multiplicative inverse} is violated.
        \item $\mathbb{Q}$ is a field.
        \item $\mathbb{R}$ is a field.
        \item $\mathbb{C}$ is a field.
        \item $\mathbb{Q}(\sqrt{2})=\{a+b\sqrt{2} \ |\ a,b \in \mathbb{Q} \}$ is a field.
        \item $\mathbb{Q}(i)=\{a+bi \ |\ a,b \in \mathbb{Q} \}$ is a field.
        \item The set \{0,1\} along with the following operations forms a field.
        \begin{multicols}{2}
            \noindent
            \begin{tabular}{l|cr}
                + & 0 & 1 \\
                \hline
                0 & 0 & 1 \\
                1 & 0 & 1
            \end{tabular}
            
            \columnbreak
            
            \noindent
            \begin{tabular}{l|cr}
                $\times$ & 0 & 1 \\
                \hline
                0 & 0 & 0 \\
                1 & 0 & 1
        \end{tabular}
        \end{multicols}
    \end{enumerate}
\end{example}

\begin{definition}[Scalar]
    Elements of a field are commonly referred as \textbf{scalars}.
\end{definition}

From now on, when we talk about a system of linear equations or a matrix, we always refer to it on a field $\mathbb{F}$, where all its coefficients, constants and solutions are elements of the field $\mathbb{F}$.

\subsection{Vector Spaces and Subspaces}
\begin{definition}[Vector Space]\label{def_vecspace}
    A vector space $\mathbb{V}$ over a field $\mathbb{F}$ is a set together with two functions, addition $+: \mathbb{V} \times \mathbb{V} \rightarrow \mathbb{V}$ and scalar multiplication $\cdot: \mathbb{F} \times \mathbb{V} \rightarrow \mathbb{V}$, s.t. the following properties hold.
    \begin{enumerate}[label=(\arabic*)]
        \item $\forall \ u,v \in \mathbb{V}, \ u+v=v+u \ (additive \ commutativity)$
        \item $\forall \ u,v,w \in \mathbb{V}, \ u+(v+w)=(u+v)+w \ (additive \ associativity)$
        \item $\exists \ 0 \mathbb{V}, \ s.t. \ \forall u \in \mathbb{V}, \ 0+u=u \ (additive \ identity)$
        \item $ \forall \ u \in \mathbb{V}, \ \exists \ (-u) \in \mathbb{V}, \ s.t. \ u+(-u)=0 \ (additive \ inverse)$
        \item $ \forall \ u \in \mathbb{V}, \ \forall \ \alpha,\beta \in \mathbb{F}, \ \alpha \cdot(\beta \cdot u)=(\alpha \cdot \beta) \cdot u \ (scalar \ multiplicative \ associativity) $
        \item $ \forall \ u \in \mathbb{V}, \ \forall \ \alpha,\beta \in \mathbb{F}, \ (\alpha+\beta) \cdot u = \alpha \cdot u + \beta \cdot u (scalar \ distributivity)$
        \item $ \forall \ u,v \in \mathbb{V}, \ \forall \ \alpha \in \mathbb{F}, \ \alpha \cdot (u+v) = \alpha \cdot u  + \alpha \cdot v \ (vector \ distributivity)$
        \item $ \forall \ u \in \mathbb{V}, \ 1 \in \mathbb{F}, \ 1 \cdot u = u \ (multiplicative \ identity)$
    \end{enumerate}
\end{definition}

\begin{example} \
    \begin{enumerate}
        \item $\mathbb{R}$, $\mathbb{R}^2$, ..., $\mathbb{R}^n$ are vector spaces over $\mathbb{R}$.
        \item Let $x$ be a real variable, the set $P_2(\mathbb{R})$ of all real polynomials in the variable $x$ of degree $\leq 2 $ is a vector space over $\mathbb{R}$. More generally, the set $P(\mathbb{R})$ of all real polynomials in the variable $x$ is a vector space over $\mathbb{R}$.
        \item Let $\mathbb{V}$ be a set of infinitely differentiable functions $f: [0,1] \rightarrow \mathbb{R}$ s.t. $f(0)=f(1)=0$, then $\mathbb{V}$ is a vector space of $\mathbb{R}$.
    \end{enumerate}
\end{example}




\begin{proposition}[Vector Cancellation Law]
   Let $\mathbb{V}$ be a vector space over a field $\mathbb{F}$, and let $u,v,w \in \mathbb{V}$, s.t. $u+v=u+w$, then $v=w$.
\end{proposition}
\begin{proof}
    In order to prove $v=w$, we start with the LHS and perform operations to achieve the RHS.
    \begin{align*}
        LHS = v &= 0 + v & \text{...(3)} \\
        &= u + (-u) + v & \text{...(4)} \\
        &= (-u) + u + v & \text{...(1)} \\
        &= (-u) + (u + v) & \text{...(2)} \\
        &= (-u) + (u + w) & \text{...(assumption)} \\
        &= (-u) + u + w & \text{...(2)} \\
        &= 0 + w & \text{...(3)} \\
        &= w = RHS & \text{...(3)}
    \end{align*}
\end{proof}


% ------------------
% Reference and Cited Works
% ------------------

\bibliography{References.bib}

% ------------------

\bibliographystyle{IEEEtran}

\end{document}
